{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path, shutil, zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 7360008803164029450,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 6529366361\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 17850940195484952484\n",
       " physical_device_desc: \"device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 폴더를 생성\n",
    "# 원 훈련 데이터가 들어있는 폴더\n",
    "original_dataset_dir = \"./datasets/\"\n",
    "\n",
    "# 일부 데이터가 들어갈 폴더 생성\n",
    "base_dir = \"./datasets/foodimage\"\n",
    "if not os.path.exists(base_dir):\n",
    "    os.mkdir(base_dir)\n",
    "    \n",
    "# ./data/foodimage/train\n",
    "# 이어지는 폴더를 연결해준다\n",
    "\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "if not os.path.exists(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "if not os.path.exists(test_dir):\n",
    "    os.mkdir(test_dir)\n",
    "validation_dir = os.path.join(base_dir, \"validation\")\n",
    "if not os.path.exists(validation_dir):\n",
    "    os.mkdir(validation_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aglio 알리오올리오\n",
    "train_aglio_dir = os.path.join(train_dir, \"aglio\")\n",
    "if not os.path.exists(train_aglio_dir):\n",
    "    os.mkdir(train_aglio_dir)\n",
    "test_aglio_dir = os.path.join(test_dir, \"aglio\")\n",
    "if not os.path.exists(test_aglio_dir):\n",
    "    os.mkdir(test_aglio_dir)\n",
    "validation_aglio_dir = os.path.join(validation_dir, \"aglio\")\n",
    "if not os.path.exists(validation_aglio_dir):\n",
    "    os.mkdir(validation_aglio_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bulgogi 불고기\n",
    "train_bulgogi_dir = os.path.join(train_dir, \"bulgogi\")\n",
    "if not os.path.exists(train_bulgogi_dir):\n",
    "    os.mkdir(train_bulgogi_dir)\n",
    "test_bulgogi_dir = os.path.join(test_dir, \"bulgogi\")\n",
    "if not os.path.exists(test_bulgogi_dir):\n",
    "    os.mkdir(test_bulgogi_dir)\n",
    "validation_bulgogi_dir = os.path.join(validation_dir, \"bulgogi\")\n",
    "if not os.path.exists(validation_bulgogi_dir):\n",
    "    os.mkdir(validation_bulgogi_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cpasta 크림파스타\n",
    "train_Cpasta_dir = os.path.join(train_dir, \"Cpasta\")\n",
    "if not os.path.exists(train_Cpasta_dir):\n",
    "    os.mkdir(train_Cpasta_dir)\n",
    "test_Cpasta_dir = os.path.join(test_dir, \"Cpasta\")\n",
    "if not os.path.exists(test_Cpasta_dir):\n",
    "    os.mkdir(test_Cpasta_dir)\n",
    "validation_Cpasta_dir = os.path.join(validation_dir, \"Cpasta\")\n",
    "if not os.path.exists(validation_Cpasta_dir):\n",
    "    os.mkdir(validation_Cpasta_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# galbijjim 갈비찜\n",
    "train_galbijjim_dir = os.path.join(train_dir, \"galbijjim\")\n",
    "if not os.path.exists(train_galbijjim_dir):\n",
    "    os.mkdir(train_galbijjim_dir)\n",
    "test_galbijjim_dir = os.path.join(test_dir, \"galbijjim\")\n",
    "if not os.path.exists(test_galbijjim_dir):\n",
    "    os.mkdir(test_galbijjim_dir)\n",
    "validation_galbijjim_dir = os.path.join(validation_dir, \"galbijjim\")\n",
    "if not os.path.exists(validation_galbijjim_dir):\n",
    "    os.mkdir(validation_galbijjim_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jabchae 잡채\n",
    "train_jabchae_dir = os.path.join(train_dir, \"jabchae\")\n",
    "if not os.path.exists(train_jabchae_dir):\n",
    "    os.mkdir(train_jabchae_dir)\n",
    "test_jabchae_dir = os.path.join(test_dir, \"jabchae\")\n",
    "if not os.path.exists(test_jabchae_dir):\n",
    "    os.mkdir(test_jabchae_dir)\n",
    "validation_jabchae_dir = os.path.join(validation_dir, \"jabchae\")\n",
    "if not os.path.exists(validation_jabchae_dir):\n",
    "    os.mkdir(validation_jabchae_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# salad 샐러드\n",
    "train_salad_dir = os.path.join(train_dir, \"salad\")\n",
    "if not os.path.exists(train_salad_dir):\n",
    "    os.mkdir(train_salad_dir)\n",
    "test_salad_dir = os.path.join(test_dir, \"salad\")\n",
    "if not os.path.exists(test_salad_dir):\n",
    "    os.mkdir(test_salad_dir)\n",
    "validation_salad_dir = os.path.join(validation_dir, \"salad\")\n",
    "if not os.path.exists(validation_salad_dir):\n",
    "    os.mkdir(validation_salad_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeaMustardSoup 미역국\n",
    "train_SeaMustardSoup_dir = os.path.join(train_dir, \"SeaMustardSoup\")\n",
    "if not os.path.exists(train_SeaMustardSoup_dir):\n",
    "    os.mkdir(train_SeaMustardSoup_dir)\n",
    "test_SeaMustardSoup_dir = os.path.join(test_dir, \"SeaMustardSoup\")\n",
    "if not os.path.exists(test_SeaMustardSoup_dir):\n",
    "    os.mkdir(test_SeaMustardSoup_dir)\n",
    "validation_SeaMustardSoup_dir = os.path.join(validation_dir, \"SeaMustardSoup\")\n",
    "if not os.path.exists(validation_SeaMustardSoup_dir):\n",
    "    os.mkdir(validation_SeaMustardSoup_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# soybean_soup 된장국\n",
    "train_soybean_soup_dir = os.path.join(train_dir, \"soybean_soup\")\n",
    "if not os.path.exists(train_soybean_soup_dir):\n",
    "    os.mkdir(train_soybean_soup_dir)\n",
    "test_soybean_soup_dir = os.path.join(test_dir, \"soybean_soup\")\n",
    "if not os.path.exists(test_soybean_soup_dir):\n",
    "    os.mkdir(test_soybean_soup_dir)\n",
    "validation_soybean_soup_dir = os.path.join(validation_dir, \"soybean_soup\")\n",
    "if not os.path.exists(validation_soybean_soup_dir):\n",
    "    os.mkdir(validation_soybean_soup_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tomatopasta 토마토파스타\n",
    "train_tomatopasta_dir = os.path.join(train_dir, \"tomatopasta\")\n",
    "if not os.path.exists(train_tomatopasta_dir):\n",
    "    os.mkdir(train_tomatopasta_dir)\n",
    "test_tomatopasta_dir = os.path.join(test_dir, \"tomatopasta\")\n",
    "if not os.path.exists(test_tomatopasta_dir):\n",
    "    os.mkdir(test_tomatopasta_dir)\n",
    "validation_tomatopasta_dir = os.path.join(validation_dir, \"tomatopasta\")\n",
    "if not os.path.exists(validation_tomatopasta_dir):\n",
    "    os.mkdir(validation_tomatopasta_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# steak 스테이크\n",
    "train_steak_dir = os.path.join(train_dir, \"steak\")\n",
    "if not os.path.exists(train_steak_dir):\n",
    "    os.mkdir(train_steak_dir)\n",
    "test_steak_dir = os.path.join(test_dir, \"steak\")\n",
    "if not os.path.exists(test_steak_dir):\n",
    "    os.mkdir(test_steak_dir)\n",
    "validation_steak_dir = os.path.join(validation_dir, \"steak\")\n",
    "if not os.path.exists(validation_steak_dir):\n",
    "    os.mkdir(validation_steak_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "traincount = range(4000)\n",
    "testcount = range(4000,4855)\n",
    "# validationcount = range(500,600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 알리오올리오 이미지 파일을 train_aglio_dir 폴더로 복사\n",
    "fnames = [\"aglio.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"aglio\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_aglio_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"aglio.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_aglio_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"aglio.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_aglio_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 불고기 이미지 파일을 train_bulgogi_dir 폴더로 복사\n",
    "fnames = [\"bulgogi.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"bulgogi\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_bulgogi_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"bulgogi.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_bulgogi_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"bulgogi.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_bulgogi_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 크림파스타 이미지 파일을 train_Cpasta_dir 폴더로 복사\n",
    "fnames = [\"Cpasta.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"Cpasta\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_Cpasta_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"Cpasta.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_Cpasta_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"Cpasta.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_Cpasta_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 갈비찜 이미지 파일을 train_galbijjim_dir 폴더로 복사\n",
    "fnames = [\"galbijjim.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"galbijjim\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_galbijjim_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"galbijjim.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_galbijjim_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"galbijjim.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_galbijjim_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 잡채 이미지 파일을 train_jabchae_dir 폴더로 복사\n",
    "fnames = [\"jabchae.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"jabchae\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_jabchae_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"jabchae.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_jabchae_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"jabchae.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_jabchae_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 샐러드 이미지 파일을 train_salad_dir 폴더로 복사\n",
    "fnames = [\"salad.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"salad\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_salad_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"salad.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_salad_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"salad.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_salad_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 미역국 이미지 파일을 train_SeaMustardSoup_dir 폴더로 복사\n",
    "fnames = [\"SeaMustardSoup.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"SeaMustardSoup\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_SeaMustardSoup_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"SeaMustardSoup.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_SeaMustardSoup_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"SeaMustardSoup.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_SeaMustardSoup_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 된장국 이미지 파일을 train_soybean_soup_dir 폴더로 복사\n",
    "fnames = [\"soybean_soup.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"soybean_soup\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_soybean_soup_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"soybean_soup.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_soybean_soup_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"soybean_soup.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_soybean_soup_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 토마토파스타 이미지 파일을 train_tomatopasta_dir 폴더로 복사\n",
    "fnames = [\"tomatopasta.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"tomatopasta\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_tomatopasta_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"tomatopasta.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_tomatopasta_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"tomatopasta.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_tomatopasta_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 스테이크 이미지 파일을 train_steak_dir 폴더로 복사\n",
    "fnames = [\"steak.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"steak\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_steak_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"steak.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_steak_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"steak.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_steak_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 픽셀값을 0-1 사이로 변환\n",
    "train_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# flow_from_directory() : 디렉토리로부터 이미지를 가져옴\n",
    "# flow_from_directory(폴더명, 이미지크기, 한번에 변화할 이미지수, 라벨링 모드)\n",
    "# 라벨링 모드 다중 분류면 categorical, 이진 분류면 binary\n",
    "# 라벨 번호는 0부터 시작 폴더명의 알파벳 순으로 할당\n",
    "train_generator = train_gen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=20, class_mode=\"categorical\")\n",
    "\n",
    "test_generator = test_gen.flow_from_directory(test_dir, target_size=(224, 224), batch_size=10, class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 증식 설정\n",
    "train_dataGen = ImageDataGenerator(rescale=1./255,\n",
    "                            rotation_range=20,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode=\"nearest\")\n",
    "\n",
    "test_dataGen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36000 images belonging to 10 classes.\n",
      "Found 12000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_dataGen.flow_from_directory(train_dir,\n",
    "                                                   target_size=(224, 224),\n",
    "                                                   batch_size=50,\n",
    "                                                   class_mode=\"categorical\")\n",
    "validation_generator = test_dataGen.flow_from_directory(test_dir,\n",
    "                                                 target_size=(224, 224),\n",
    "                                                 batch_size=50,\n",
    "                                                 class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cpasta': 0, 'SeaMustardSoup': 1, 'aglio': 2, 'bulgogi': 3, 'galbijjim': 4, 'jabchae': 5, 'salad': 6, 'soybean_soup': 7, 'steak': 8, 'tomatopasta': 9}\n",
      "{'Cpasta': 0, 'SeaMustardSoup': 1, 'aglio': 2, 'bulgogi': 3, 'galbijjim': 4, 'jabchae': 5, 'salad': 6, 'soybean_soup': 7, 'steak': 8, 'tomatopasta': 9}\n"
     ]
    }
   ],
   "source": [
    "# 라벨링 결과 확인\n",
    "print(train_generator.class_indices)\n",
    "print(validation_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), input_shape=(224, 224, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(64, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(128, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(256, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(512, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               3211392   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 17,927,370\n",
      "Trainable params: 17,927,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.applications import VGG16\n",
    "conv_base=VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "model = Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "model = Sequential()\n",
    "resn=ResNet50V2(weights=None, include_top=False, input_shape=(224,224,3))\n",
    "model.add(resn)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# 동결되기 전의 훈련되는 가중치의 수\n",
    "print(len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1\n",
      "block1_conv1\n",
      "block1_conv2\n",
      "block1_pool\n",
      "block2_conv1\n",
      "block2_conv2\n",
      "block2_pool\n",
      "block3_conv1\n",
      "block3_conv2\n",
      "block3_conv3\n",
      "block3_pool\n",
      "block4_conv1\n",
      "block4_conv2\n",
      "block4_conv3\n",
      "block4_pool\n",
      "block5_conv1\n",
      "block5_conv2\n",
      "block5_conv3\n",
      "block5_pool\n"
     ]
    }
   ],
   "source": [
    "for layer in conv_base.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 층까지 모든 층 동결하기\n",
    "\n",
    "conv_base.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == \"block5_conv1\" or layer.name == \"block5_conv1\" or layer.name == \"block5_conv3\":\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# 동결 후의 훈련되는 가중치의 수\n",
    "print(len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               3211392   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 17,927,370\n",
      "Trainable params: 10,292,106\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"acc\"], optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h = model.fit(train_generator, steps_per_epoch=20, epochs=100, validation_data=test_generator, validation_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50000\n",
      "199/720 [=======>......................] - ETA: 4:25 - loss: 2.3865 - acc: 0.0980"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "# 모델을 저장할 폴더명\n",
    "MODEL_FOLDER = \"./model/\"\n",
    "\n",
    "# 해당 폴더가 없다면 해당 폴더를 생성\n",
    "if not os.path.exists(MODEL_FOLDER):\n",
    "    os.mkdir(MODEL_FOLDER)\n",
    "    \n",
    "# 저장할 파일명 설정\n",
    "# {epoch:04d} : 반복수를 4자리로 표시\n",
    "# {acc:.4f} : 정확도를 소수점 4째자리까지 표시\n",
    "modelpath = MODEL_FOLDER + \"foodvgg16-210216-{epoch:04d}-{val_acc:.4f}.hdf5\"\n",
    "\n",
    "# 베스트를 찾아서 해당 파일명으로 저장\n",
    "# save_best_only : 더 나은 결과값만 저장\n",
    "# ModelCheckpoint(filepath=파일패스, monitor=기준값, save_best_only=True/False)\n",
    "mc = ModelCheckpoint(filepath=modelpath, monitor=\"val_acc\", save_best_only=True, verbose=1)\n",
    "\n",
    "# EarlyStopping(monitor=기준값, patience=조금 더 기다리는횟수)\n",
    "# patience=20 : 학습이 더 나아지지 않더라도 20회는 더 반복해줌\n",
    "es = EarlyStopping(monitor=\"val_acc\", patience=50)\n",
    "\n",
    "# 학습\n",
    "h = model.fit(train_generator, steps_per_epoch=None, epochs=50000, validation_data=validation_generator, validation_steps=None, callbacks=[mc, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=h.history[\"acc\"]\n",
    "val_acc=h.history[\"val_acc\"]\n",
    "\n",
    "epochs = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epochs, acc, c=\"red\", label=\"train acc\")\n",
    "plt.plot(epochs, val_acc, c=\"green\", label=\"validation acc\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "img_path = './datasets/tomatopasta/tomatopasta.399.png'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "img_data = image.img_to_array(img)\n",
    "img_data = np.expand_dims(img_data, axis=0)\n",
    "#img_data = preprocess_input(img_data)\n",
    "\n",
    "pred = model.predict(img_data)\n",
    "\n",
    "# 결과 디코딩해서 튜플로 반환 (class, description, probability)\n",
    "ind=np.where(pred[0]==pred[0].max())[0][0]\n",
    "result=list(train_generator.class_indices.keys())[ind]\n",
    "print(\"예측:\",pred)\n",
    "print('결과:', result)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "img_path = './datasets/tomatopasta/tomatopasta.602.png'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "img_data = image.img_to_array(img)\n",
    "img_data = np.expand_dims(img_data, axis=0)\n",
    "#img_data = preprocess_input(img_data)\n",
    "\n",
    "pred = model2.predict(img_data)\n",
    "\n",
    "# 결과 디코딩해서 튜플로 반환 (class, description, probability)\n",
    "ind=np.where(pred[0]==pred[0].max())[0][0]\n",
    "result=list(train_generator.class_indices.keys())[ind]\n",
    "print(\"예측:\",pred)\n",
    "print('결과:', result)\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
