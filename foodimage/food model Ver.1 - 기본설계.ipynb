{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path, shutil, zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 14011984524405262574,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 6529366361\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 12759662534353871488\n",
       " physical_device_desc: \"device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 폴더를 생성\n",
    "# 원 훈련 데이터가 들어있는 폴더\n",
    "original_dataset_dir = \"./datasets/\"\n",
    "\n",
    "# 일부 데이터가 들어갈 폴더 생성\n",
    "base_dir = \"./datasets/foodimage\"\n",
    "if not os.path.exists(base_dir):\n",
    "    os.mkdir(base_dir)\n",
    "    \n",
    "# ./data/foodimage/train\n",
    "# 이어지는 폴더를 연결해준다\n",
    "\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "if not os.path.exists(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "if not os.path.exists(test_dir):\n",
    "    os.mkdir(test_dir)\n",
    "validation_dir = os.path.join(base_dir, \"validation\")\n",
    "if not os.path.exists(validation_dir):\n",
    "    os.mkdir(validation_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aglio 알리오올리오\n",
    "train_aglio_dir = os.path.join(train_dir, \"aglio\")\n",
    "if not os.path.exists(train_aglio_dir):\n",
    "    os.mkdir(train_aglio_dir)\n",
    "test_aglio_dir = os.path.join(test_dir, \"aglio\")\n",
    "if not os.path.exists(test_aglio_dir):\n",
    "    os.mkdir(test_aglio_dir)\n",
    "validation_aglio_dir = os.path.join(validation_dir, \"aglio\")\n",
    "if not os.path.exists(validation_aglio_dir):\n",
    "    os.mkdir(validation_aglio_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bulgogi 불고기\n",
    "train_bulgogi_dir = os.path.join(train_dir, \"bulgogi\")\n",
    "if not os.path.exists(train_bulgogi_dir):\n",
    "    os.mkdir(train_bulgogi_dir)\n",
    "test_bulgogi_dir = os.path.join(test_dir, \"bulgogi\")\n",
    "if not os.path.exists(test_bulgogi_dir):\n",
    "    os.mkdir(test_bulgogi_dir)\n",
    "validation_bulgogi_dir = os.path.join(validation_dir, \"bulgogi\")\n",
    "if not os.path.exists(validation_bulgogi_dir):\n",
    "    os.mkdir(validation_bulgogi_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cpasta 크림파스타\n",
    "train_Cpasta_dir = os.path.join(train_dir, \"Cpasta\")\n",
    "if not os.path.exists(train_Cpasta_dir):\n",
    "    os.mkdir(train_Cpasta_dir)\n",
    "test_Cpasta_dir = os.path.join(test_dir, \"Cpasta\")\n",
    "if not os.path.exists(test_Cpasta_dir):\n",
    "    os.mkdir(test_Cpasta_dir)\n",
    "validation_Cpasta_dir = os.path.join(validation_dir, \"Cpasta\")\n",
    "if not os.path.exists(validation_Cpasta_dir):\n",
    "    os.mkdir(validation_Cpasta_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# galbijjim 갈비찜\n",
    "train_galbijjim_dir = os.path.join(train_dir, \"galbijjim\")\n",
    "if not os.path.exists(train_galbijjim_dir):\n",
    "    os.mkdir(train_galbijjim_dir)\n",
    "test_galbijjim_dir = os.path.join(test_dir, \"galbijjim\")\n",
    "if not os.path.exists(test_galbijjim_dir):\n",
    "    os.mkdir(test_galbijjim_dir)\n",
    "validation_galbijjim_dir = os.path.join(validation_dir, \"galbijjim\")\n",
    "if not os.path.exists(validation_galbijjim_dir):\n",
    "    os.mkdir(validation_galbijjim_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jabchae 잡채\n",
    "train_jabchae_dir = os.path.join(train_dir, \"jabchae\")\n",
    "if not os.path.exists(train_jabchae_dir):\n",
    "    os.mkdir(train_jabchae_dir)\n",
    "test_jabchae_dir = os.path.join(test_dir, \"jabchae\")\n",
    "if not os.path.exists(test_jabchae_dir):\n",
    "    os.mkdir(test_jabchae_dir)\n",
    "validation_jabchae_dir = os.path.join(validation_dir, \"jabchae\")\n",
    "if not os.path.exists(validation_jabchae_dir):\n",
    "    os.mkdir(validation_jabchae_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# salad 샐러드\n",
    "train_salad_dir = os.path.join(train_dir, \"salad\")\n",
    "if not os.path.exists(train_salad_dir):\n",
    "    os.mkdir(train_salad_dir)\n",
    "test_salad_dir = os.path.join(test_dir, \"salad\")\n",
    "if not os.path.exists(test_salad_dir):\n",
    "    os.mkdir(test_salad_dir)\n",
    "validation_salad_dir = os.path.join(validation_dir, \"salad\")\n",
    "if not os.path.exists(validation_salad_dir):\n",
    "    os.mkdir(validation_salad_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeaMustardSoup 미역국\n",
    "train_SeaMustardSoup_dir = os.path.join(train_dir, \"SeaMustardSoup\")\n",
    "if not os.path.exists(train_SeaMustardSoup_dir):\n",
    "    os.mkdir(train_SeaMustardSoup_dir)\n",
    "test_SeaMustardSoup_dir = os.path.join(test_dir, \"SeaMustardSoup\")\n",
    "if not os.path.exists(test_SeaMustardSoup_dir):\n",
    "    os.mkdir(test_SeaMustardSoup_dir)\n",
    "validation_SeaMustardSoup_dir = os.path.join(validation_dir, \"SeaMustardSoup\")\n",
    "if not os.path.exists(validation_SeaMustardSoup_dir):\n",
    "    os.mkdir(validation_SeaMustardSoup_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# soybean_soup 된장국\n",
    "train_soybean_soup_dir = os.path.join(train_dir, \"soybean_soup\")\n",
    "if not os.path.exists(train_soybean_soup_dir):\n",
    "    os.mkdir(train_soybean_soup_dir)\n",
    "test_soybean_soup_dir = os.path.join(test_dir, \"soybean_soup\")\n",
    "if not os.path.exists(test_soybean_soup_dir):\n",
    "    os.mkdir(test_soybean_soup_dir)\n",
    "validation_soybean_soup_dir = os.path.join(validation_dir, \"soybean_soup\")\n",
    "if not os.path.exists(validation_soybean_soup_dir):\n",
    "    os.mkdir(validation_soybean_soup_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tomatopasta 토마토파스타\n",
    "train_tomatopasta_dir = os.path.join(train_dir, \"tomatopasta\")\n",
    "if not os.path.exists(train_tomatopasta_dir):\n",
    "    os.mkdir(train_tomatopasta_dir)\n",
    "test_tomatopasta_dir = os.path.join(test_dir, \"tomatopasta\")\n",
    "if not os.path.exists(test_tomatopasta_dir):\n",
    "    os.mkdir(test_tomatopasta_dir)\n",
    "validation_tomatopasta_dir = os.path.join(validation_dir, \"tomatopasta\")\n",
    "if not os.path.exists(validation_tomatopasta_dir):\n",
    "    os.mkdir(validation_tomatopasta_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# steak 스테이크\n",
    "train_steak_dir = os.path.join(train_dir, \"steak\")\n",
    "if not os.path.exists(train_steak_dir):\n",
    "    os.mkdir(train_steak_dir)\n",
    "test_steak_dir = os.path.join(test_dir, \"steak\")\n",
    "if not os.path.exists(test_steak_dir):\n",
    "    os.mkdir(test_steak_dir)\n",
    "validation_steak_dir = os.path.join(validation_dir, \"steak\")\n",
    "if not os.path.exists(validation_steak_dir):\n",
    "    os.mkdir(validation_steak_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traincount = range(4000)\n",
    "testcount = range(4000,4855)\n",
    "# validationcount = range(500,600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 알리오올리오 이미지 파일을 train_aglio_dir 폴더로 복사\n",
    "fnames = [\"aglio.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"aglio\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_aglio_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"aglio.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_aglio_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"aglio.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_aglio_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 불고기 이미지 파일을 train_bulgogi_dir 폴더로 복사\n",
    "fnames = [\"bulgogi.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"bulgogi\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_bulgogi_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"bulgogi.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_bulgogi_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"bulgogi.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_bulgogi_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 크림파스타 이미지 파일을 train_Cpasta_dir 폴더로 복사\n",
    "fnames = [\"Cpasta.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"Cpasta\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_Cpasta_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"Cpasta.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_Cpasta_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"Cpasta.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_Cpasta_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 갈비찜 이미지 파일을 train_galbijjim_dir 폴더로 복사\n",
    "fnames = [\"galbijjim.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"galbijjim\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_galbijjim_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"galbijjim.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_galbijjim_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"galbijjim.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_galbijjim_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 잡채 이미지 파일을 train_jabchae_dir 폴더로 복사\n",
    "fnames = [\"jabchae.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"jabchae\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_jabchae_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"jabchae.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_jabchae_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"jabchae.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_jabchae_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 샐러드 이미지 파일을 train_salad_dir 폴더로 복사\n",
    "fnames = [\"salad.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"salad\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_salad_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"salad.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_salad_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"salad.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_salad_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 미역국 이미지 파일을 train_SeaMustardSoup_dir 폴더로 복사\n",
    "fnames = [\"SeaMustardSoup.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"SeaMustardSoup\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_SeaMustardSoup_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"SeaMustardSoup.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_SeaMustardSoup_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"SeaMustardSoup.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_SeaMustardSoup_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 된장국 이미지 파일을 train_soybean_soup_dir 폴더로 복사\n",
    "fnames = [\"soybean_soup.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"soybean_soup\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_soybean_soup_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"soybean_soup.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_soybean_soup_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"soybean_soup.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_soybean_soup_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 토마토파스타 이미지 파일을 train_tomatopasta_dir 폴더로 복사\n",
    "fnames = [\"tomatopasta.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"tomatopasta\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_tomatopasta_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"tomatopasta.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_tomatopasta_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"tomatopasta.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_tomatopasta_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 처음부터 1000개의 스테이크 이미지 파일을 train_steak_dir 폴더로 복사\n",
    "fnames = [\"steak.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"steak\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_steak_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"steak.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_steak_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "# fnames = [\"steak.{}.png\".format(i) for i in validationcount]\n",
    "\n",
    "# for fname in fnames:\n",
    "#     # 원래 이미지가 있는 폴더와 파읾명\n",
    "#     src = os.path.join(food_dataset_dir, fname)\n",
    "#     # 복사할 위치의 폴더와 파일명\n",
    "#     dst = os.path.join(validation_steak_dir, fname)\n",
    "#     shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 픽셀값을 0-1 사이로 변환\n",
    "train_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# flow_from_directory() : 디렉토리로부터 이미지를 가져옴\n",
    "# flow_from_directory(폴더명, 이미지크기, 한번에 변화할 이미지수, 라벨링 모드)\n",
    "# 라벨링 모드 다중 분류면 categorical, 이진 분류면 binary\n",
    "# 라벨 번호는 0부터 시작 폴더명의 알파벳 순으로 할당\n",
    "train_generator = train_gen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=20, class_mode=\"categorical\")\n",
    "\n",
    "test_generator = test_gen.flow_from_directory(test_dir, target_size=(224, 224), batch_size=10, class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 증식 설정\n",
    "train_dataGen = ImageDataGenerator(rescale=1./255,\n",
    "                            rotation_range=20,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode=\"nearest\")\n",
    "\n",
    "test_dataGen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36000 images belonging to 10 classes.\n",
      "Found 12000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_dataGen.flow_from_directory(train_dir,\n",
    "                                                   target_size=(224, 224),\n",
    "                                                   batch_size=50,\n",
    "                                                   class_mode=\"categorical\")\n",
    "test_generator = test_dataGen.flow_from_directory(test_dir,\n",
    "                                                 target_size=(224, 224),\n",
    "                                                 batch_size=50,\n",
    "                                                 class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cpasta': 0, 'SeaMustardSoup': 1, 'aglio': 2, 'bulgogi': 3, 'galbijjim': 4, 'jabchae': 5, 'salad': 6, 'soybean_soup': 7, 'steak': 8, 'tomatopasta': 9}\n",
      "{'Cpasta': 0, 'SeaMustardSoup': 1, 'aglio': 2, 'bulgogi': 3, 'galbijjim': 4, 'jabchae': 5, 'salad': 6, 'soybean_soup': 7, 'steak': 8, 'tomatopasta': 9}\n"
     ]
    }
   ],
   "source": [
    "# 라벨링 결과 확인\n",
    "print(train_generator.class_indices)\n",
    "print(test_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 218, 218, 32)      4736      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 109, 109, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 105, 105, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 52, 52, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               10240128  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 10,371,274\n",
      "Trainable params: 10,371,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (7,7), input_shape=(224, 224, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(64, (5,5), activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(128, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.applications import VGG16\n",
    "conv_base=VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "model = Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "model = Sequential()\n",
    "resn=ResNet50V2(weights=None, include_top=False, input_shape=(224,224,3))\n",
    "model.add(resn)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# 동결되기 전의 훈련되는 가중치의 수\n",
    "print(len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resn.trainable = True\n",
    "set_trainable = False\n",
    "for layer in resn.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 특정 층까지 모든 층 동결하기\n",
    "\n",
    "conv_base.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == \"block5_conv1\":\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 동결 후의 훈련되는 가중치의 수\n",
    "print(len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00277615583366\n",
    "# lr = 0.00181503843077\n",
    "# lr = 0.00163685841542\n",
    "# lr = 0.00122869861281\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"acc\"], optimizer=tf.keras.optimizers.Adam(lr=lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h = model.fit(train_generator, steps_per_epoch=100, epochs=100, validation_data=test_generator, validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50000\n",
      "720/720 [==============================] - 369s 503ms/step - loss: 2.5384 - acc: 0.1002 - val_loss: 2.3029 - val_acc: 0.1000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.10000, saving model to ./model\\foodstandard210217-0001-0.1000.hdf5\n",
      "Epoch 2/50000\n",
      " 63/720 [=>............................] - ETA: 4:48 - loss: 2.3027 - acc: 0.0991"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-60cb02833428>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "# 모델을 저장할 폴더명\n",
    "MODEL_FOLDER = \"./model/\"\n",
    "\n",
    "# 해당 폴더가 없다면 해당 폴더를 생성\n",
    "if not os.path.exists(MODEL_FOLDER):\n",
    "    os.mkdir(MODEL_FOLDER)\n",
    "    \n",
    "# 저장할 파일명 설정\n",
    "# {epoch:04d} : 반복수를 4자리로 표시\n",
    "# {acc:.4f} : 정확도를 소수점 4째자리까지 표시\n",
    "modelpath = MODEL_FOLDER + \"foodstandard210217-{epoch:04d}-{val_acc:.4f}.hdf5\"\n",
    "\n",
    "# 베스트를 찾아서 해당 파일명으로 저장\n",
    "# save_best_only : 더 나은 결과값만 저장\n",
    "# ModelCheckpoint(filepath=파일패스, monitor=기준값, save_best_only=True/False)\n",
    "mc = ModelCheckpoint(filepath=modelpath, monitor=\"val_acc\", save_best_only=True, verbose=1, mode=\"max\")\n",
    "\n",
    "# EarlyStopping(monitor=기준값, patience=조금 더 기다리는횟수)\n",
    "# patience=20 : 학습이 더 나아지지 않더라도 20회는 더 반복해줌\n",
    "es = EarlyStopping(monitor=\"val_acc\", patience=10)\n",
    "\n",
    "# 학습\n",
    "h = model.fit(train_generator, steps_per_epoch=None, epochs=50000, validation_data=test_generator, validation_steps=None, callbacks=[mc, es], verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=h.history[\"acc\"]\n",
    "val_acc=h.history[\"val_acc\"]\n",
    "\n",
    "epochs = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epochs, acc, c=\"red\", label=\"train acc\")\n",
    "plt.plot(epochs, val_acc, c=\"green\", label=\"validation acc\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model2 = load_model(\"./model/foodstandard-0063-0.8583.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "img_path = './datasets/tomatopasta/tomatopasta.3000.png'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "img_data = image.img_to_array(img)\n",
    "img_data = np.expand_dims(img_data, axis=0)\n",
    "#img_data = preprocess_input(img_data)\n",
    "\n",
    "pred = model2.predict(img_data)\n",
    "\n",
    "# 결과 디코딩해서 튜플로 반환 (class, description, probability)\n",
    "ind=np.where(pred[0]==pred[0].max())[0][0]\n",
    "result=list(train_generator.class_indices.keys())[ind]\n",
    "print(\"예측:\",pred)\n",
    "print('결과:', result)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "img_path = './datasets/tomatopasta/tomatopasta.602.png'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "img_data = image.img_to_array(img)\n",
    "img_data = np.expand_dims(img_data, axis=0)\n",
    "#img_data = preprocess_input(img_data)\n",
    "\n",
    "pred = model2.predict(img_data)\n",
    "\n",
    "# 결과 디코딩해서 튜플로 반환 (class, description, probability)\n",
    "ind=np.where(pred[0]==pred[0].max())[0][0]\n",
    "result=list(train_generator.class_indices.keys())[ind]\n",
    "print(\"예측:\",pred)\n",
    "print('결과:', result)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
