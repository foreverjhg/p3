{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 음식 사진을 올려 음식을 판별하는 모델\n",
    "\n",
    "- 데이터 수집\n",
    "    - 한식(5개)\n",
    "        - 갈비찜, 불고기, 미역국, 된장국, 잡채\n",
    "    - 양식(5개)\n",
    "        - 알리오올리오, 크림파스타, 토마토파스타, 샐러드, 스테이크\n",
    "        \n",
    "- 음식 사진 각각 3600개(훈련), 1200개(검증) 사용\n",
    "\n",
    "- 모델 설계 (현재 기준입니다!)\n",
    "    1. 기본 설계 : 85% 정확도\n",
    "    2. resnet50 : 90% 정확도\n",
    "    3. Xception : 93% 정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 수집 및 전처리\n",
    "\n",
    "훈련, 테스트 디렉토리 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path, shutil, zipfile\n",
    "\n",
    "# 각 폴더를 생성\n",
    "# 원 훈련 데이터가 들어있는 폴더\n",
    "original_dataset_dir = \"./datasets/\"\n",
    "\n",
    "# 일부 데이터가 들어갈 폴더 생성\n",
    "base_dir = \"./datasets/foodimage\"\n",
    "if not os.path.exists(base_dir):\n",
    "    os.mkdir(base_dir)\n",
    "    \n",
    "# ./data/foodimage/train\n",
    "# 이어지는 폴더를 연결해준다\n",
    "\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "if not os.path.exists(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "if not os.path.exists(test_dir):\n",
    "    os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 검증 개수 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "traincount = range(3600)\n",
    "testcount = range(3600,4800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설정한 개수에 맞게 미리 한 폴더에 넣어놓은 음식 사진들을 훈련 검증 데이터로 나누기(최초 1회만 실행)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aglio 알리오올리오\n",
    "# 훈련 검증 폴더 지정(없으면 만들기)\n",
    "train_aglio_dir = os.path.join(train_dir, \"aglio\")\n",
    "if not os.path.exists(train_aglio_dir):\n",
    "    os.mkdir(train_aglio_dir)\n",
    "test_aglio_dir = os.path.join(test_dir, \"aglio\")\n",
    "if not os.path.exists(test_aglio_dir):\n",
    "    os.mkdir(test_aglio_dir)\n",
    "\n",
    "# 훈련 데이터 생성\n",
    "fnames = [\"aglio.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"aglio\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_aglio_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "\n",
    "# 검증 데이터 생성\n",
    "fnames = [\"aglio.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_aglio_dir, fname)\n",
    "    shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bulgogi 불고기\n",
    "train_bulgogi_dir = os.path.join(train_dir, \"bulgogi\")\n",
    "if not os.path.exists(train_bulgogi_dir):\n",
    "    os.mkdir(train_bulgogi_dir)\n",
    "test_bulgogi_dir = os.path.join(test_dir, \"bulgogi\")\n",
    "if not os.path.exists(test_bulgogi_dir):\n",
    "    os.mkdir(test_bulgogi_dir)\n",
    "    \n",
    "fnames = [\"bulgogi.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"bulgogi\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_bulgogi_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"bulgogi.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_bulgogi_dir, fname)\n",
    "    shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cpasta 크림파스타\n",
    "train_Cpasta_dir = os.path.join(train_dir, \"Cpasta\")\n",
    "if not os.path.exists(train_Cpasta_dir):\n",
    "    os.mkdir(train_Cpasta_dir)\n",
    "test_Cpasta_dir = os.path.join(test_dir, \"Cpasta\")\n",
    "if not os.path.exists(test_Cpasta_dir):\n",
    "    os.mkdir(test_Cpasta_dir)\n",
    "    \n",
    "fnames = [\"Cpasta.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"Cpasta\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_Cpasta_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"Cpasta.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_Cpasta_dir, fname)\n",
    "    shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# galbijjim 갈비찜\n",
    "train_galbijjim_dir = os.path.join(train_dir, \"galbijjim\")\n",
    "if not os.path.exists(train_galbijjim_dir):\n",
    "    os.mkdir(train_galbijjim_dir)\n",
    "test_galbijjim_dir = os.path.join(test_dir, \"galbijjim\")\n",
    "if not os.path.exists(test_galbijjim_dir):\n",
    "    os.mkdir(test_galbijjim_dir\n",
    "             \n",
    "            fnames = [\"galbijjim.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"galbijjim\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_galbijjim_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"galbijjim.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_galbijjim_dir, fname)\n",
    "    shutil.copyfile(src,dst)ㅍ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jabchae 잡채\n",
    "train_jabchae_dir = os.path.join(train_dir, \"jabchae\")\n",
    "if not os.path.exists(train_jabchae_dir):\n",
    "    os.mkdir(train_jabchae_dir)\n",
    "test_jabchae_dir = os.path.join(test_dir, \"jabchae\")\n",
    "if not os.path.exists(test_jabchae_dir):\n",
    "    os.mkdir(test_jabchae_dir)\n",
    "    \n",
    "fnames = [\"jabchae.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"jabchae\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_jabchae_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"jabchae.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_jabchae_dir, fname)\n",
    "    shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# salad 샐러드\n",
    "train_salad_dir = os.path.join(train_dir, \"salad\")\n",
    "if not os.path.exists(train_salad_dir):\n",
    "    os.mkdir(train_salad_dir)\n",
    "test_salad_dir = os.path.join(test_dir, \"salad\")\n",
    "if not os.path.exists(test_salad_dir):\n",
    "    os.mkdir(test_salad_dir)\n",
    "    \n",
    "fnames = [\"salad.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"salad\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_salad_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"salad.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_salad_dir, fname)\n",
    "    shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeaMustardSoup 미역국\n",
    "train_SeaMustardSoup_dir = os.path.join(train_dir, \"SeaMustardSoup\")\n",
    "if not os.path.exists(train_SeaMustardSoup_dir):\n",
    "    os.mkdir(train_SeaMustardSoup_dir)\n",
    "test_SeaMustardSoup_dir = os.path.join(test_dir, \"SeaMustardSoup\")\n",
    "if not os.path.exists(test_SeaMustardSoup_dir):\n",
    "    os.mkdir(test_SeaMustardSoup_dir)\n",
    "    \n",
    "fnames = [\"SeaMustardSoup.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"SeaMustardSoup\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_SeaMustardSoup_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"SeaMustardSoup.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_SeaMustardSoup_dir, fname)\n",
    "    shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# soybean_soup 된장국\n",
    "train_soybean_soup_dir = os.path.join(train_dir, \"soybean_soup\")\n",
    "if not os.path.exists(train_soybean_soup_dir):\n",
    "    os.mkdir(train_soybean_soup_dir)\n",
    "test_soybean_soup_dir = os.path.join(test_dir, \"soybean_soup\")\n",
    "if not os.path.exists(test_soybean_soup_dir):\n",
    "    os.mkdir(test_soybean_soup_dir)\n",
    "    \n",
    "fnames = [\"soybean_soup.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"soybean_soup\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_soybean_soup_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"soybean_soup.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_soybean_soup_dir, fname)\n",
    "    shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tomatopasta 토마토파스타\n",
    "train_tomatopasta_dir = os.path.join(train_dir, \"tomatopasta\")\n",
    "if not os.path.exists(train_tomatopasta_dir):\n",
    "    os.mkdir(train_tomatopasta_dir)\n",
    "test_tomatopasta_dir = os.path.join(test_dir, \"tomatopasta\")\n",
    "if not os.path.exists(test_tomatopasta_dir):\n",
    "    os.mkdir(test_tomatopasta_dir)\n",
    "    \n",
    "fnames = [\"tomatopasta.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"tomatopasta\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_tomatopasta_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"tomatopasta.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_tomatopasta_dir, fname)\n",
    "    shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# steak 스테이크\n",
    "train_steak_dir = os.path.join(train_dir, \"steak\")\n",
    "if not os.path.exists(train_steak_dir):\n",
    "    os.mkdir(train_steak_dir)\n",
    "test_steak_dir = os.path.join(test_dir, \"steak\")\n",
    "if not os.path.exists(test_steak_dir):\n",
    "    os.mkdir(test_steak_dir)\n",
    "    \n",
    "fnames = [\"steak.{}.png\".format(i) for i in traincount]\n",
    "\n",
    "food_dataset_dir = os.path.join(original_dataset_dir, \"steak\")\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_steak_dir, fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = [\"steak.{}.png\".format(i) for i in testcount]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파읾명\n",
    "    src = os.path.join(food_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_steak_dir, fname)\n",
    "    shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 증식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 증식 설정\n",
    "train_dataGen = ImageDataGenerator(rescale=1./255,\n",
    "                            rotation_range=20,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode=\"nearest\")\n",
    "\n",
    "test_dataGen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 36000개 훈련 이미지, 12000개 검증 이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36000 images belonging to 10 classes.\n",
      "Found 12000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_dataGen.flow_from_directory(train_dir,\n",
    "                                                   target_size=(224, 224),\n",
    "                                                   batch_size=50,\n",
    "                                                   class_mode=\"categorical\")\n",
    "validation_generator = test_dataGen.flow_from_directory(test_dir,\n",
    "                                                 target_size=(224, 224),\n",
    "                                                 batch_size=50,\n",
    "                                                 class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라벨링 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cpasta': 0, 'SeaMustardSoup': 1, 'aglio': 2, 'bulgogi': 3, 'galbijjim': 4, 'jabchae': 5, 'salad': 6, 'soybean_soup': 7, 'steak': 8, 'tomatopasta': 9}\n",
      "{'Cpasta': 0, 'SeaMustardSoup': 1, 'aglio': 2, 'bulgogi': 3, 'galbijjim': 4, 'jabchae': 5, 'salad': 6, 'soybean_soup': 7, 'steak': 8, 'tomatopasta': 9}\n"
     ]
    }
   ],
   "source": [
    "print(train_generator.class_indices)\n",
    "print(validation_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  모델 설계\n",
    "\n",
    "- 가장 성능이 좋았던 Xception 전이학습 모델\n",
    "- imagenet 가중치를 받아와서 상위 층 레이어만 훈련 가능하도록 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMT021\\anaconda3\\envs\\deep\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\SMT021\\anaconda3\\envs\\deep\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "NASNet (Functional)          (None, 7, 7, 1056)        4269716   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 51744)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               6623360   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 10,894,366\n",
      "Trainable params: 10,857,628\n",
      "Non-trainable params: 36,738\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.applications import NASNetMobile\n",
    "\n",
    "nas_base=NASNetMobile(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "model = Sequential()\n",
    "model.add(nas_base)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.applications import Xception\n",
    "\n",
    "xcep_base=Xception(weights=None, include_top=False, input_shape=(224, 224, 3))\n",
    "model = Sequential()\n",
    "model.add(xcep_base)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "744\n"
     ]
    }
   ],
   "source": [
    "# 동결되기 전의 훈련되는 가중치의 수\n",
    "print(len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14번째 블럭 레이어들은 동결 해제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nas_base.trainable = True\n",
    "set_trainable = False\n",
    "for layer in nas_base.layers:\n",
    "    if layer.name[:7] == \"block14\":\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 동결 후의 훈련되는 가중치의 수\n",
    "print(len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 컴파일\n",
    "- 학습률(lr) 조정으로 성능 업그레이드 확인중입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 0.00277615583366\n",
    "# lr = 0.00181503843077\n",
    "# lr = 0.00163685841542\n",
    "# lr = 0.00122869861281\n",
    "lr = 0.001\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"acc\"], optimizer=tf.keras.optimizers.Adam(lr=lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 fit\n",
    "- 검증 정확도가 10번 나아지지 않으면 멈추도록 earlystopping 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50000\n",
      "  1/720 [..............................] - ETA: 4:42:06 - loss: 2.9445 - acc: 0.1000"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "# 모델을 저장할 폴더명\n",
    "MODEL_FOLDER = \"D:/model/\"\n",
    "\n",
    "# 해당 폴더가 없다면 해당 폴더를 생성\n",
    "if not os.path.exists(MODEL_FOLDER):\n",
    "    os.mkdir(MODEL_FOLDER)\n",
    "    \n",
    "# 저장할 파일명 설정\n",
    "# {epoch:04d} : 반복수를 4자리로 표시\n",
    "# {acc:.4f} : 정확도를 소수점 4째자리까지 표시\n",
    "modelpath = MODEL_FOLDER + \"foodnasnet210218-{epoch:04d}-{val_acc:.4f}.hdf5\"\n",
    "\n",
    "# 베스트를 찾아서 해당 파일명으로 저장\n",
    "# save_best_only : 더 나은 결과값만 저장\n",
    "# ModelCheckpoint(filepath=파일패스, monitor=기준값, save_best_only=True/False)\n",
    "mc = ModelCheckpoint(filepath=modelpath, monitor=\"val_acc\", save_best_only=True, verbose=1)\n",
    "\n",
    "# EarlyStopping(monitor=기준값, patience=조금 더 기다리는횟수)\n",
    "# patience=20 : 학습이 더 나아지지 않더라도 10회는 더 반복해줌\n",
    "es = EarlyStopping(monitor=\"val_acc\", patience=10)\n",
    "\n",
    "# 학습\n",
    "h = model.fit(train_generator, steps_per_epoch=None, epochs=50000, validation_data=validation_generator, validation_steps=None, callbacks=[mc, es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도 그래프 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=h.history[\"acc\"]\n",
    "val_acc=h.history[\"val_acc\"]\n",
    "\n",
    "epochs = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epochs, acc, c=\"red\", label=\"train acc\")\n",
    "plt.plot(epochs, val_acc, c=\"green\", label=\"validation acc\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model2 = load_model(\"./model/foodxception_trainable-0024-0.9208.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검증 데이터에 있는 이미지로 실제 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_string_from_index(index):\n",
    "    for class_string, class_index in validation_generator.class_indices.items():\n",
    "        if class_index == index:\n",
    "            return class_string\n",
    "\n",
    "x, y = next(validation_generator)\n",
    "image = x[2, :, :, :]\n",
    "true_index = np.argmax(y[2])\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Expand the validation image to (1, 224, 224, 3) before predicting the label\n",
    "prediction_scores = model2.predict(np.expand_dims(image, axis=0))\n",
    "predicted_index = np.argmax(prediction_scores)\n",
    "print(\"True label: \" + get_class_string_from_index(true_index))\n",
    "print(\"Predicted label: \" + get_class_string_from_index(predicted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
